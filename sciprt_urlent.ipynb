{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, Dropout, Input, Reshape\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import yaml\n",
    "\n",
    "def loss_wrapper(weights_loss):\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        return tf.nn.softmax_cross_entropy_with_logits()\n",
    "def get_yamldict(yaml_path):\n",
    "    with open(yaml_path, \"r\", encoding=\"utf-8\") as yaml_file:\n",
    "        yaml_dict = yaml.load(yaml_file, Loader=yaml.Loader)\n",
    "    return yaml_dict\n",
    "def getModel(word_seq_len, char_seq_len, ngram_seq_len, chars_dict_len, word_dict_len, ngram_dict_len, reg_lambda,\n",
    "             emb_dim):\n",
    "    ########### char-level embedding, conv layer #############\n",
    "    char_input = Input(shape=(char_seq_len), name=\"char_input\")\n",
    "    print(\"shape of char_input: \", char_input.shape)\n",
    "    emb_char = Embedding(chars_dict_len, emb_dim)(char_input)\n",
    "    print(\"char-Embedding: \", emb_char.shape)\n",
    "    pooled_result = []\n",
    "    for filter_size in [3, 4, 5, 6]:\n",
    "        char_conv_output = Conv1D(filters=256, kernel_size=filter_size, strides=1, activation=\"relu\", padding=\"VALID\")(\n",
    "            emb_char)\n",
    "        print(\"char-convOutput : \", char_conv_output.shape)\n",
    "        max_pooled_output = MaxPooling1D(pool_size=char_seq_len  - filter_size + 1, strides=1, padding=\"VALID\")(\n",
    "            char_conv_output)\n",
    "        print(\"char - maxPool shape: \", max_pooled_output.shape)\n",
    "        pooled_result.append(max_pooled_output)\n",
    "    num_filters_total = 256 * 4\n",
    "    filter_pooled = tensorflow.concat(pooled_result, axis=1)\n",
    "    print(\"4 filter output pooled: \", filter_pooled.shape)\n",
    "    char_flat = tensorflow.reshape(filter_pooled, [-1, num_filters_total])\n",
    "    char_drop_out = Dropout(0.5)(char_flat)\n",
    "    ######### word-level embedding, conv_layer ###########\n",
    "    input_word = Input(shape=(word_seq_len), name=\"word_input\")\n",
    "    input_ngram = Input(shape=(word_seq_len * ngram_seq_len), name=\"ngram_input\")\n",
    "    reshaping_layer = Reshape((word_seq_len, ngram_seq_len), input_shape=(word_seq_len * ngram_seq_len,))\n",
    "    reshepd_ngram = reshaping_layer(input_ngram)\n",
    "    # input_ngram_padded = Input(shape=(200,20,32,), name=\"ngram_padded_input\")\n",
    "    emb_word = Embedding(word_dict_len + 1, emb_dim)(input_word)\n",
    "    print(\"word-embedding: \", emb_word.shape)\n",
    "    emb_ngram = Embedding(ngram_dict_len + 1, emb_dim)(reshepd_ngram)\n",
    "    emb_ngram_sum = tensorflow.reduce_sum(emb_ngram, 2)\n",
    "    tot_word_emb = tensorflow.add(emb_ngram_sum, emb_word)\n",
    "    word_pooled_result = []\n",
    "    for filter_size in [3, 4, 5, 6]:\n",
    "        layer_name = \"word_layer_fsize_\" + str(filter_size)\n",
    "        word_conv_output = Conv1D(filters=256, kernel_size=filter_size, strides=1, activation=\"relu\", padding=\"VALID\",\n",
    "                                  name=layer_name)(tot_word_emb)\n",
    "        max_pooled_output_word = MaxPooling1D(pool_size=word_seq_len - filter_size + 1, strides=1, padding=\"VALID\")(\n",
    "            word_conv_output)\n",
    "        word_pooled_result.append(max_pooled_output_word)\n",
    "    word_filter_pooled = tensorflow.concat(word_pooled_result, axis=1)\n",
    "    word_flat = tensorflow.reshape(word_filter_pooled, [-1, num_filters_total])\n",
    "    print(\"word_flat shape: \", word_flat.shape)\n",
    "    word_drop_out = Dropout(0.5)(word_flat)\n",
    "    ############# Fully connected Layer ##########\n",
    "    char_dense_layer = Dense(512, input_shape=(num_filters_total,), kernel_initializer=\"glorot_uniform\",\n",
    "                             kernel_regularizer=l2(reg_lambda))(char_drop_out)\n",
    "    word_dense_layer = Dense(512, input_shape=(num_filters_total,), kernel_initializer=\"glorot_uniform\",\n",
    "                             kernel_regularizer=l2(reg_lambda))(word_drop_out)\n",
    "    conv_final_output = tensorflow.concat([char_dense_layer, word_dense_layer], 1)\n",
    "    ############### Dense layer - before_softmax #######################\n",
    "    d0 = Dense(512, input_shape=(1024,), kernel_initializer=\"glorot_uniform\", kernel_regularizer=l2(reg_lambda))(\n",
    "        conv_final_output)\n",
    "    d1 = Dense(256, input_shape=(512,), kernel_initializer=\"glorot_uniform\", kernel_regularizer=l2(reg_lambda))(d0)\n",
    "    d2 = Dense(128, input_shape=(256,), kernel_initializer=\"glorot_uniform\", kernel_regularizer=l2(reg_lambda))(d1)\n",
    "    score = Dense(2, input_shape=(128,), kernel_initializer=\"glorot_uniform\", kernel_regularizer=l2(reg_lambda))(d2)\n",
    "    ############### Score & Predictions & Probability ##############\n",
    "    predictions = tensorflow.argmax(score, 1, name=\"predictions\")\n",
    "    prob = tensorflow.nn.softmax(score, name=\"prob\")\n",
    "    model = tensorflow.keras.Model([char_input, input_word, input_ngram], prob)\n",
    "    return model\n",
    "\n",
    "def input_fn(path, maxWordPerUrl, maxCharPerUrl, maxNgramPerUrl, shuffle_buffer_size, batch_size):\n",
    "    dataset = tensorflow.data.TFRecordDataset(path)\n",
    "    feature_map = {\n",
    "        \"engineeredChar\": tensorflow.io.FixedLenFeature((maxCharPerUrl,), tensorflow.int64),\n",
    "        \"engineeredWord\": tensorflow.io.FixedLenFeature((maxWordPerUrl,), tensorflow.int64),\n",
    "        \"engineeredNgram\": tensorflow.io.FixedLenFeature((maxWordPerUrl * maxNgramPerUrl,), tensorflow.int64),\n",
    "        \"label\": tensorflow.io.FixedLenFeature([], tensorflow.int64)\n",
    "    }\n",
    "    def _parse_fn(record, feature_map):\n",
    "        example = tensorflow.io.parse_single_example(serialized=record, features=feature_map)\n",
    "        onehot_label = tensorflow.cast(tensorflow.one_hot(tensorflow.where(tensorflow.equal(example[\"label\"], 1), 1, 0), depth=2), dtype=tensorflow.int64)\n",
    "        return {\"char_input\": example[\"engineeredChar\"], \"word_input\": example[\"engineeredWord\"],\n",
    "                \"ngram_input\": example[\"engineeredNgram\"]}, onehot_label\n",
    "    dataset = dataset.map(lambda record: _parse_fn(record, feature_map))\n",
    "    return dataset.shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "def read_tfrecord(path, maxWordPerUrl, maxCharPerUrl, maxNgramPerUrl, read_batch_size):\n",
    "    dataset = tensorflow.data.TFRecordDataset(path)\n",
    "    feature_map = {\n",
    "        \"engineeredChar\": tensorflow.io.FixedLenFeature((maxCharPerUrl,), tensorflow.int64),\n",
    "        \"engineeredWord\": tensorflow.io.FixedLenFeature((maxWordPerUrl,), tensorflow.int64),\n",
    "        \"engineeredNgram\": tensorflow.io.FixedLenFeature((maxWordPerUrl * maxNgramPerUrl,), tensorflow.int64),\n",
    "        \"label\": tensorflow.io.FixedLenFeature([], tensorflow.int64)\n",
    "    }\n",
    "    def _parse_fn(record, feature_map):\n",
    "        example = tensorflow.io.parse_single_example(serialized=record, features=feature_map)\n",
    "        onehot_label = tensorflow.cast(tensorflow.one_hot(tensorflow.where(tensorflow.equal(example[\"label\"], 1), 1, 0), depth=2), dtype=tensorflow.int64)\n",
    "        return {\"char_input\": example[\"engineeredChar\"], \"word_input\": example[\"engineeredWord\"],\n",
    "                \"ngram_input\": example[\"engineeredNgram\"]}, onehot_label\n",
    "    dataset = dataset.map(lambda record: _parse_fn(record, feature_map))\n",
    "    return dataset.batch(read_batch_size)\n",
    " \n",
    "def input_small_fn(path, maxWordPerUrl, maxCharPerUrl, maxNgramPerUrl):\n",
    "    dataset = tensorflow.data.TFRecordDataset(path)\n",
    "    feature_map = {\n",
    "        \"engineeredChar\": tensorflow.io.FixedLenFeature((maxCharPerUrl,), tensorflow.int64),\n",
    "        \"engineeredWord\": tensorflow.io.FixedLenFeature((maxWordPerUrl,), tensorflow.int64),\n",
    "        \"engineeredNgram\": tensorflow.io.FixedLenFeature((maxWordPerUrl * maxNgramPerUrl,), tensorflow.int64),\n",
    "        \"label\": tensorflow.io.FixedLenFeature([], tensorflow.int64)\n",
    "    }\n",
    "    def _parse_fn(record, feature_map):\n",
    "        example = tensorflow.io.parse_single_example(serialized=record, features=feature_map)\n",
    "        onehot_label = tensorflow.cast(tensorflow.one_hot(tensorflow.where(tensorflow.equal(example[\"label\"], 1), 1, 0), depth=2), dtype=tensorflow.int64)\n",
    "        return {\"char_input\": example[\"engineeredChar\"], \"word_input\": example[\"engineeredWord\"],\n",
    "                \"ngram_input\": example[\"engineeredNgram\"]}, onehot_label\n",
    "    dataset = dataset.map(lambda record: _parse_fn(record, feature_map))\n",
    "    return dataset.take(10)\n",
    " \n",
    "def get_label_and_urlstring(path):\n",
    "    dataset = tensorflow.data.TFRecordDataset(path)\n",
    "    feature_map = {\n",
    "        \"url\": tensorflow.io.FixedLenFeature([], tensorflow.string),\n",
    "        \"label\":tensorflow.io.FixedLenFeature([], tensorflow.int64),\n",
    "    }\n",
    "    def _parse_fn(record, feature_map):\n",
    "        example = tensorflow.io.parse_single_example(serialized=record, features=feature_map)\n",
    "        one_hot_label = tensorflow.cast(tensorflow.one_hot(tensorflow.where(tensorflow.equal(example[\"label\"], 1),1,0), depth=2), dtype=tensorflow.int64)\n",
    "        return {\"url\": example[\"url\"], \"label\": one_hot_label}\n",
    "    \n",
    "    return dataset.map(lambda record: _parse_fn(record, feature_map))\n",
    "    \n",
    "    \n",
    "def get_label_url_source(path):\n",
    "    dataset = tensorflow.data.TFRecordDataset(path)\n",
    "    feature_map = {\n",
    "        \"url\": tensorflow.io.FixedLenFeature([], tensorflow.string),\n",
    "        \"label\":tensorflow.io.FixedLenFeature([], tensorflow.int64),\n",
    "        \"urlSource\":tensorflow.io.FixedLenFeature([], tensorflow.string)\n",
    "    }\n",
    "    def _parse_fn(record, feature_map):\n",
    "        example = tensorflow.io.parse_single_example(serialized=record, features=feature_map)\n",
    "        one_hot_label = tensorflow.cast(tensorflow.one_hot(tensorflow.where(tensorflow.equal(example[\"label\"], 1),1,0), depth=2), dtype=tensorflow.int64)\n",
    "        return {\"url\": example[\"url\"], \"label\": one_hot_label, \"urlSource\": example[\"urlSource\"]}\n",
    "    \n",
    "    dataset = dataset.map(lambda record: _parse_fn(record, feature_map))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(path):\n",
    "    result={}\n",
    "    tmp = pd.read_parquet(path)\n",
    "    for token, index in zip(tmp.iloc[:,0], tmp.iloc[:,1]):\n",
    "        result[index]=token\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = make_dict(\"/home/youngjai/sampleData/ailab/workspace/youngjai_kwon/pml-data/engineered-data/totalSet/2020-11-09/small_dictionary/wordTokenIndex\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pd.read_parquet(\"/home/youngjai/sampleData/ailab/workspace/youngjai_kwon/pml-data/engineered-data/totalSet/2020-11-09/small_dictionary/charTokenIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq_len = 200\n",
    "char_seq_len = 200\n",
    "ngram_seq_len = 20\n",
    "chars_dict_len = 1406 \n",
    "word_dict_len = 10507\n",
    "ngram_dict_len = 1406\n",
    "reg_lambda = 0 \n",
    "emb_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of char_input:  (None, 200)\n",
      "char-Embedding:  (None, 200, 32)\n",
      "char-convOutput :  (None, 198, 256)\n",
      "char - maxPool shape:  (None, 1, 256)\n",
      "char-convOutput :  (None, 197, 256)\n",
      "char - maxPool shape:  (None, 1, 256)\n",
      "char-convOutput :  (None, 196, 256)\n",
      "char - maxPool shape:  (None, 1, 256)\n",
      "char-convOutput :  (None, 195, 256)\n",
      "char - maxPool shape:  (None, 1, 256)\n",
      "4 filter output pooled:  (None, 4, 256)\n",
      "word-embedding:  (None, 200, 32)\n",
      "word_flat shape:  (None, 1024)\n"
     ]
    }
   ],
   "source": [
    "model = getModel(word_seq_len, char_seq_len, ngram_seq_len, chars_dict_len, word_dict_len, ngram_dict_len, reg_lambda, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_dir = \"/home/youngjai/sampleData/ailab/workspace/youngjai_kwon/pml-data/engineered-data/totalSet/2020-11-09/small_splitted/train.tfrecord/part-r-\"\n",
    "train_data_dir = [train_base_dir+str(idx).zfill(5) for idx in range(5)]\n",
    "\n",
    "val_base_dir =  \"/home/youngjai/sampleData/ailab/workspace/youngjai_kwon/pml-data/engineered-data/totalSet/2020-11-09/small_splitted/test.tfrecord/part-r-\"\n",
    "val_data_dir = [val_base_dir+str(idx).zfill(5) for idx in range(5)]\n",
    "\n",
    "oop_base_dir = \"/home/youngjai/sampleData/ailab/workspace/youngjai_kwon/pml-data/engineered-data/totalSet/2020-11-09/small_oop/tfrecords\"\n",
    "oop_data_dir = [oop_base_dir + str(idx).zfill(5) for idx in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = input_fn(train_data_dir, word_seq_len, char_seq_len, ngram_seq_len, 10000, 256)\n",
    "val_dataset = input_fn(val_data_dir, word_seq_len, char_seq_len, ngram_seq_len, 10000, 256)\n",
    "oop_dataset = input_fn(oop_data_dir, word_seq_len, char_seq_len, ngram_seq_len, 10000, 256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oop_test_dataset = input_small_fn(oop_data_dir, word_seq_len, char_seq_len, ngram_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ({char_input: (200,), word_input: (200,), ngram_input: (4000,)}, (2,)), types: ({char_input: tf.int64, word_input: tf.int64, ngram_input: tf.int64}, tf.int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.unbatch().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tr_dataset, epochs=3, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f9422a17d73e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/youngjai/sampleData/ailab/workspace/youngjai_kwon/pml-data/model/2020-11-09/small_70k\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"/home/youngjai/sampleData/ailab/workspace/youngjai_kwon/pml-data/model/2020-11-09/small_70k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of char_input:  (None, 200)\n",
      "char-Embedding:  (None, 200, 32)\n",
      "char-convOutput :  (None, 198, 256)\n",
      "char - maxPool shape:  (None, 1, 256)\n",
      "char-convOutput :  (None, 197, 256)\n",
      "char - maxPool shape:  (None, 1, 256)\n",
      "char-convOutput :  (None, 196, 256)\n",
      "char - maxPool shape:  (None, 1, 256)\n",
      "char-convOutput :  (None, 195, 256)\n",
      "char - maxPool shape:  (None, 1, 256)\n",
      "4 filter output pooled:  (None, 4, 256)\n",
      "word-embedding:  (None, 200, 32)\n",
      "word_flat shape:  (None, 1024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7efbcc50c828>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = getModel(word_seq_len, char_seq_len, ngram_seq_len, chars_dict_len, word_dict_len, ngram_dict_len, reg_lambda, emb_dim)\n",
    "model.load_weights(\"/home/youngjai/sampleData/ailab/workspace/youngjai_kwon/pml-data/model/2020-11-09/small_70k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_wt = model.get_layer(\"word_layer_fsize_3\").weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'word_layer_fsize_3/kernel:0' shape=(3, 32, 256) dtype=float32, numpy=\n",
       " array([[[ 4.37490419e-02,  4.04972695e-02,  6.83686808e-02, ...,\n",
       "          -2.94406768e-02,  2.14401856e-02, -7.07160309e-02],\n",
       "         [-5.03828079e-02,  8.21285620e-02,  8.11735634e-03, ...,\n",
       "          -5.17946742e-02,  6.70022368e-02, -5.20262942e-02],\n",
       "         [-5.69256023e-02,  7.66882300e-02,  2.32748836e-02, ...,\n",
       "           6.02978878e-02,  6.99732155e-02, -1.93480458e-02],\n",
       "         ...,\n",
       "         [ 2.08949335e-02,  8.45661089e-02, -6.97435662e-02, ...,\n",
       "           3.94282378e-02, -5.96826710e-02,  6.03995733e-02],\n",
       "         [-7.13708550e-02,  3.20183225e-02, -4.33528656e-03, ...,\n",
       "           5.30924164e-02, -7.34575838e-02,  4.28760387e-02],\n",
       "         [ 1.72489919e-02,  6.88444823e-04,  4.73778276e-03, ...,\n",
       "          -4.49759476e-02, -3.55219888e-03, -1.47054419e-01]],\n",
       " \n",
       "        [[-6.38278574e-02, -2.71720346e-02,  3.16900611e-02, ...,\n",
       "          -8.55979845e-02,  4.52550463e-02, -6.42086938e-02],\n",
       "         [ 4.91653271e-02, -1.25993993e-02, -5.35281524e-02, ...,\n",
       "           8.12993050e-02, -8.13529566e-02, -6.87973201e-02],\n",
       "         [ 4.48750146e-02,  2.19494104e-02,  2.04350874e-02, ...,\n",
       "          -7.40787154e-03,  7.26689249e-02,  2.65750121e-02],\n",
       "         ...,\n",
       "         [-5.88568114e-02,  7.84977451e-02,  2.64153238e-02, ...,\n",
       "          -2.32257061e-02,  3.31320353e-02, -2.24479772e-02],\n",
       "         [-5.84889837e-02, -4.04022485e-02, -6.21841289e-02, ...,\n",
       "           7.28494376e-02,  6.93149716e-02, -4.74430308e-05],\n",
       "         [ 5.99132143e-02,  1.51609816e-03, -1.92113463e-02, ...,\n",
       "          -6.06796443e-02, -5.69874868e-02, -3.18041220e-02]],\n",
       " \n",
       "        [[ 5.59213907e-02,  6.30278736e-02, -1.52479773e-02, ...,\n",
       "          -7.68723190e-02,  5.72706386e-02, -1.18736913e-02],\n",
       "         [ 3.53863761e-02,  2.40762271e-02,  1.17842533e-01, ...,\n",
       "          -8.45358893e-02, -3.25051462e-03,  1.45623893e-01],\n",
       "         [ 4.54232506e-02, -2.91617401e-02, -2.76711565e-02, ...,\n",
       "           4.09082808e-02, -4.60947193e-02,  7.30468184e-02],\n",
       "         ...,\n",
       "         [ 7.90836662e-02,  3.71552445e-02,  3.41790169e-02, ...,\n",
       "          -5.72857596e-02,  3.03034182e-03,  1.37874745e-02],\n",
       "         [-3.66956927e-02,  4.88904975e-02, -1.58495624e-02, ...,\n",
       "          -5.81015311e-02, -6.43101111e-02,  7.27247968e-02],\n",
       "         [ 3.87971662e-02, -7.65769556e-02, -4.31726873e-02, ...,\n",
       "          -5.85587211e-02,  2.78116614e-02,  5.14201894e-02]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'word_layer_fsize_3/bias:0' shape=(256,) dtype=float32, numpy=\n",
       " array([-1.75627172e-02, -1.18646743e-02, -2.18851864e-02, -1.03449123e-02,\n",
       "        -3.04605369e-03, -1.00709978e-04, -3.66296843e-02, -1.07361302e-02,\n",
       "        -9.41539183e-03, -3.25335399e-03, -1.56954844e-02, -4.63166554e-03,\n",
       "        -5.34325326e-03, -3.44730653e-02, -1.39441462e-02, -1.67921260e-02,\n",
       "        -1.25648146e-02, -8.32964666e-03,  7.87754171e-03, -1.41132828e-02,\n",
       "         2.84405053e-03, -1.41486796e-02, -5.35094589e-02,  4.91411053e-03,\n",
       "        -4.72133327e-03, -1.65114086e-02, -2.33045779e-02, -2.38979911e-03,\n",
       "        -2.30683945e-02, -1.60359796e-02,  1.76273286e-03, -2.51820702e-02,\n",
       "        -1.64008569e-02, -4.50983783e-03, -1.98570974e-02,  5.27001033e-03,\n",
       "        -2.06247922e-02, -1.43190818e-02, -1.38822002e-02, -1.68486089e-02,\n",
       "        -2.27034427e-02, -1.52874207e-02,  9.02013481e-03, -6.22971952e-02,\n",
       "         7.37618469e-03, -6.57652621e-04,  8.91007297e-03, -5.64290304e-03,\n",
       "        -2.13589631e-02, -5.48540205e-02, -3.01739178e-03,  1.49707627e-02,\n",
       "        -9.00655892e-03, -1.01114614e-02, -1.83930583e-02, -5.36844321e-03,\n",
       "        -2.29172074e-04, -1.65023301e-02, -2.05101874e-02, -1.90560948e-02,\n",
       "        -3.47980745e-02, -1.57729182e-02, -4.40943390e-02, -2.30479077e-03,\n",
       "        -1.13369180e-02, -4.46165260e-03, -1.07008200e-02, -8.37536249e-03,\n",
       "        -2.01802440e-02, -1.74363479e-02, -5.13828173e-02, -1.64310113e-02,\n",
       "        -1.02993790e-02, -1.29419127e-02, -1.60390437e-02,  1.03192236e-02,\n",
       "        -1.34930937e-02, -2.11121365e-02, -8.87120329e-03, -1.85496919e-02,\n",
       "        -6.84248330e-03, -1.75531227e-02, -2.77331788e-02, -8.86187889e-03,\n",
       "         4.79537714e-03, -3.07695568e-03, -1.36497999e-02,  6.11421419e-03,\n",
       "        -9.21571907e-03, -1.56844826e-03, -9.30724107e-03, -2.20367648e-02,\n",
       "        -2.46275123e-02, -7.43606221e-03, -3.10205910e-02, -1.15282442e-02,\n",
       "        -1.06901489e-02, -1.76515505e-02, -1.42083764e-02, -1.86656304e-02,\n",
       "        -9.89967398e-03, -1.00725302e-02, -1.82132982e-02, -1.10675320e-02,\n",
       "        -6.46225968e-03, -3.55415754e-02, -1.13470564e-02, -2.05222168e-03,\n",
       "         4.85164206e-03, -4.01587738e-03, -8.20445362e-03, -4.08855779e-03,\n",
       "         8.93745921e-04, -1.14413239e-02, -1.43645806e-02,  8.73431191e-03,\n",
       "         5.77117922e-03, -1.17291622e-02, -3.50132920e-02, -9.98915453e-03,\n",
       "        -6.91697840e-03,  1.42003724e-03,  4.50880313e-03,  2.71457550e-03,\n",
       "         5.13538835e-04,  1.22978473e-02, -8.06023367e-03, -3.07021309e-02,\n",
       "        -7.46294903e-03, -2.62369923e-02, -4.01286818e-02,  1.18681404e-03,\n",
       "        -1.24276876e-02, -1.72664411e-03, -2.46713739e-02, -1.55370100e-04,\n",
       "         1.50401648e-02,  3.93040199e-03, -8.06210842e-03, -1.29975649e-02,\n",
       "        -8.93432368e-03, -1.37122795e-02, -9.40508861e-03, -2.30489951e-02,\n",
       "        -6.55982038e-03,  3.08222161e-03, -8.01744591e-03, -8.89687147e-03,\n",
       "        -1.98284686e-02, -4.98220604e-03, -4.76823514e-03, -2.26710774e-02,\n",
       "        -1.91322714e-02, -1.02469958e-02, -3.46727408e-02,  6.29694108e-03,\n",
       "        -3.22000775e-03, -1.68836098e-02, -3.79736070e-03, -1.64375044e-02,\n",
       "        -8.72157107e-05, -2.12005433e-02, -2.12397296e-02, -2.88928300e-03,\n",
       "         2.08793711e-02, -8.15016590e-03, -2.24864930e-02, -1.41255474e-02,\n",
       "        -1.75417326e-02, -9.04148351e-03, -2.29270998e-02, -9.59973503e-03,\n",
       "        -1.50362672e-02, -4.32871701e-03, -3.05869337e-03, -5.49266599e-02,\n",
       "        -3.31105404e-02, -4.15917300e-03, -6.00586459e-03, -8.99194274e-03,\n",
       "        -2.75303051e-02,  7.54781021e-03,  3.29877366e-03, -1.44433891e-02,\n",
       "        -1.65836550e-02, -1.91025995e-02, -7.14661228e-03,  4.29097237e-03,\n",
       "        -4.37457450e-02,  7.74809811e-03, -3.24663357e-03, -1.00809569e-02,\n",
       "        -1.56637616e-02,  1.12641156e-02, -7.69562088e-03, -2.04859227e-02,\n",
       "        -4.17498052e-02, -1.37223219e-02, -3.75947617e-02, -1.01527553e-02,\n",
       "        -1.40996734e-02,  2.75322376e-03, -6.77088648e-03, -2.52459031e-02,\n",
       "        -2.20874362e-02, -1.06470892e-02, -1.52496453e-02, -1.61367208e-02,\n",
       "        -1.72232036e-02,  2.83897948e-03, -2.34390534e-02, -1.04683451e-02,\n",
       "         3.29168211e-03,  3.30484472e-03, -2.39934549e-02,  3.20308260e-03,\n",
       "        -1.48940906e-02, -7.09336298e-03,  8.48005805e-03, -4.39801067e-03,\n",
       "        -1.84098892e-02,  1.78275425e-02, -2.84640994e-02, -1.10577769e-03,\n",
       "        -1.20763527e-02, -9.80705675e-03, -9.53556504e-03, -1.21303350e-02,\n",
       "        -1.19316876e-02, -9.77663044e-03, -7.59754376e-03, -3.36159468e-02,\n",
       "        -9.38323326e-03,  6.12958102e-03, -5.35219582e-03, -1.06467698e-02,\n",
       "        -1.58267450e-02, -1.47628726e-03, -7.22779986e-03, -2.10415088e-02,\n",
       "        -1.95915010e-02, -1.24775385e-02, -1.61486659e-02, -1.08286561e-02,\n",
       "        -4.27136291e-03, -5.52230095e-03,  5.94634190e-03, -4.83026821e-03,\n",
       "        -1.90207059e-03, -2.01849062e-02, -3.39392237e-02, -3.31141464e-02,\n",
       "        -2.45956611e-02, -4.93677706e-03, -2.14164127e-02,  8.43950827e-03],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_4:0' shape=(None,) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_base_dir =  \"/home/youngjai/sampleData/ailab/workspace/youngjai_kwon/pml-data/engineered-data/totalSet/2020-11-09/small_splitted/test.tfrecord/part-r-\"\n",
    "val_data_dir = [val_base_dir+str(idx).zfill(5) for idx in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_char = []\n",
    "engineered_ngram=[]\n",
    "engineered_word=[]\n",
    "for x,_ in input_small_fn(val_data_dir, 200, 200, 20):\n",
    "    engineered_char.append(x[\"char_input\"].numpy())\n",
    "    engineered_word.append(x[\"word_input\"].numpy())\n",
    "    engineered_ngram.append(x[\"ngram_input\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list=[]\n",
    "label=[]\n",
    "source=[]\n",
    "for x in get_label_url_source(val_data_dir):\n",
    "    url_list.append(x[\"url\"].numpy().decode(\"utf-8\"))\n",
    "    label.append(np.argmax(x[\"label\"].numpy()))\n",
    "    source.append(x[\"urlSource\"].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(engineered_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "output_0 = model.output[:,0]\n",
    "output_1 = model.output[:,1]\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = model.output[:,0]\n",
    "    conv_output = model.get_layer(\"word_layer_fsize_3\").weights\n",
    "grads = tape.gradient(loss, conv_output)[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord_v2(path, maxWordPerUrl, maxCharPerUrl, maxNgramPerUrl):\n",
    "    dataset = tensorflow.data.TFRecordDataset(path)\n",
    "    feature_map = {\n",
    "        \"engineeredChar\": tensorflow.io.FixedLenFeature((maxCharPerUrl,), tensorflow.int64),\n",
    "        \"engineeredWord\": tensorflow.io.FixedLenFeature((maxWordPerUrl,), tensorflow.int64),\n",
    "        \"engineeredNgram\": tensorflow.io.FixedLenFeature((maxWordPerUrl * maxNgramPerUrl,), tensorflow.int64),\n",
    "        \"label\": tensorflow.io.FixedLenFeature([], tensorflow.int64)\n",
    "    }\n",
    "    def _parse_fn(record, feature_map):\n",
    "        example = tensorflow.io.parse_single_example(serialized=record, features=feature_map)\n",
    "        onehot_label = tensorflow.cast(tensorflow.one_hot(tensorflow.where(tensorflow.equal(example[\"label\"], 1), 1, 0), depth=2), dtype=tensorflow.int64)\n",
    "        return {\"char_input\": example[\"engineeredChar\"], \"word_input\": example[\"engineeredWord\"],\n",
    "                \"ngram_input\": example[\"engineeredNgram\"]}, onehot_label\n",
    "    dataset = dataset.batch(1).map(lambda record: _parse_fn(record, feature_map)).make_one_shot_iterator()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'char_input_1:0' shape=(None, 200) dtype=float32>,\n",
       " <tf.Tensor 'word_input_1:0' shape=(None, 200) dtype=float32>,\n",
       " <tf.Tensor 'ngram_input_1:0' shape=(None, 4000) dtype=float32>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4170, 1199, 6133,   10, 2103, 8856,   10, 5292,   10,   10,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]),\n",
       " array([ 4170, 10182,    10,  1409,  1726,  7866, 10182,    10,  2926,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]),\n",
       " array([4170, 2547,   21,   10, 2926,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered_word[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "char_tensor = tf.convert_to_tensor(engineered_char)\n",
    "ngram_tensor = tf.convert_to_tensor(engineered_ngram)\n",
    "word_tensor = tf.convert_to_tensor(engineered_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4400), dtype=int32, numpy=\n",
       "array([[56, 68, 68, ...,  0,  0,  0],\n",
       "       [56, 68, 68, ...,  0,  0,  0],\n",
       "       [56, 68, 68, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [56, 68, 68, ...,  0,  0,  0],\n",
       "       [56, 68, 68, ...,  0,  0,  0],\n",
       "       [56, 68, 68, ...,  0,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = [char_tensor, ngram_tensor, word_tensor]\n",
    "tf.concat(dd, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_input = {\"char_input\":char_tensor, \"word_input\":word_tensor, \"ngram_tensor\":ngram_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(10, 200), dtype=int32, numpy=\n",
       " array([[56, 68, 68, ...,  0,  0,  0],\n",
       "        [56, 68, 68, ...,  0,  0,  0],\n",
       "        [56, 68, 68, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [56, 68, 68, ...,  0,  0,  0],\n",
       "        [56, 68, 68, ...,  0,  0,  0],\n",
       "        [56, 68, 68, ...,  0,  0,  0]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 4000), dtype=int32, numpy=\n",
       " array([[56, 68, 68, ...,  0,  0,  0],\n",
       "        [56, 68, 68, ...,  0,  0,  0],\n",
       "        [56, 68, 68, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [56, 68, 68, ...,  0,  0,  0],\n",
       "        [56, 68, 68, ...,  0,  0,  0],\n",
       "        [56, 68, 68, ...,  0,  0,  0]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 200), dtype=int32, numpy=\n",
       " array([[ 4170,  1199,  6133, ...,     0,     0,     0],\n",
       "        [ 4170, 10182,    10, ...,     0,     0,     0],\n",
       "        [ 4170,  2547,    21, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 4170,    10,  2926, ...,     0,     0,     0],\n",
       "        [ 4170,  3202,    10, ...,     0,     0,     0],\n",
       "        [ 4170,  6891,    10, ...,     0,     0,     0]], dtype=int32)>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_input = [char_tensor, ngram_tensor, word_tensor]\n",
    "tot_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_model = tf.keras.models.Model(\n",
    "    [model.inputs], [model.get_layer(\"word_layer_fsize_3\").output, model.output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 10, 200), dtype=int32, numpy=\n",
       " array([[[56, 68, 68, ...,  0,  0,  0],\n",
       "         [56, 68, 68, ...,  0,  0,  0],\n",
       "         [56, 68, 68, ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [56, 68, 68, ...,  0,  0,  0],\n",
       "         [56, 68, 68, ...,  0,  0,  0],\n",
       "         [56, 68, 68, ...,  0,  0,  0]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 10, 4000), dtype=int32, numpy=\n",
       " array([[[56, 68, 68, ...,  0,  0,  0],\n",
       "         [56, 68, 68, ...,  0,  0,  0],\n",
       "         [56, 68, 68, ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [56, 68, 68, ...,  0,  0,  0],\n",
       "         [56, 68, 68, ...,  0,  0,  0],\n",
       "         [56, 68, 68, ...,  0,  0,  0]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 10, 200), dtype=int32, numpy=\n",
       " array([[[ 4170,  1199,  6133, ...,     0,     0,     0],\n",
       "         [ 4170, 10182,    10, ...,     0,     0,     0],\n",
       "         [ 4170,  2547,    21, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 4170,    10,  2926, ...,     0,     0,     0],\n",
       "         [ 4170,  3202,    10, ...,     0,     0,     0],\n",
       "         [ 4170,  6891,    10, ...,     0,     0,     0]]], dtype=int32)>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'char_input:0' shape=(None, 200) dtype=float32>,\n",
       " <tf.Tensor 'word_input:0' shape=(None, 200) dtype=float32>,\n",
       " <tf.Tensor 'ngram_input:0' shape=(None, 4000) dtype=float32>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(idx, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # conv_outputs 는 198*256짜리 : kernel_size3으로하면 그렇다. 그러한 filter의 갯수가 256개가 있으므로.. 196*256\n",
    "    # predictions 는 이게 정상인지 악성인지 . \n",
    "        conv_outputs, predictions = grad_model([char_tensor, word_tensor, ngram_tensor])\n",
    "        loss = predictions[:, label]\n",
    "    grads = tape.gradient(loss,conv_outputs) \n",
    "    guided_grads = tf.cast(conv_outputs > 0, \"float32\") * tf.cast(grads > 0, \"float32\") * grads\n",
    "    weight = tf.reduce_sum(guided_grads, axis=(1,2))\n",
    "    \n",
    "    ttmp = tf.reduce_sum(tf.multiply(weight[idx], conv_outputs[idx]),axis=1)\n",
    "    output = ttmp.numpy()\n",
    "    print(url_list[idx], \"  :  \", label[idx], \"\\n\")\n",
    "    for i in range(20):\n",
    "        idx2 = engineered_word[idx][i]\n",
    "        word = word_dict.get(idx2)\n",
    "        prob = output[i]\n",
    "        print(word,\" \",prob,\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tmp = tf.reduce_sum(tf.multiply(weight[0], conv_outputs[0]),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = final_tmp.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://0c606n6zcfrbk6db5t.mailone.loan/fvae444mfa903   :   1\n"
     ]
    }
   ],
   "source": [
    "print(url_list[0],\"  :  \", label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http   0.5558802 \n",
      "\n",
      "c   0.40524948 \n",
      "\n",
      "n   0.41920495 \n",
      "\n",
      "<UNK>   0.19065289 \n",
      "\n",
      "db   0.21377629 \n",
      "\n",
      "t   0.43992442 \n",
      "\n",
      "<UNK>   0.46625742 \n",
      "\n",
      "loan   0.5371562 \n",
      "\n",
      "<UNK>   0.32146972 \n",
      "\n",
      "<UNK>   0.3018909 \n",
      "\n",
      "<PAD>   0.3249948 \n",
      "\n",
      "<PAD>   0.3249948 \n",
      "\n",
      "<PAD>   0.3249948 \n",
      "\n",
      "<PAD>   0.3249948 \n",
      "\n",
      "<PAD>   0.3249948 \n",
      "\n",
      "<PAD>   0.3249948 \n",
      "\n",
      "<PAD>   0.3249948 \n",
      "\n",
      "<PAD>   0.3249948 \n",
      "\n",
      "<PAD>   0.3249948 \n",
      "\n",
      "<PAD>   0.3249948 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    idx = engineered_word[0][i]\n",
    "    word = word_dict.get(idx)\n",
    "    prob = output[i]\n",
    "    print(word,\" \",prob,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4170, 1199, 6133,   10, 2103, 8856,   10, 5292,   10,   10,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axis = 2 not in [-2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-9ccd4b68eb1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_tmp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pythonVenv/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonVenv/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mexpanded_num_dims\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mexpanded_num_dims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m       raise ValueError(\"axis = %d not in [%d, %d)\" %\n\u001b[0;32m-> 1253\u001b[0;31m                        (axis, -expanded_num_dims, expanded_num_dims))\n\u001b[0m\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axis = 2 not in [-2, 2)"
     ]
    }
   ],
   "source": [
    "tf.stack([final_tmp], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(198, 256), dtype=float32, numpy=\n",
       "array([[ 0.       , -0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.0281344],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       ...,\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guided_grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(198, 256), dtype=float32, numpy=\n",
       "array([[0.        , 0.        , 0.        , ..., 0.02633833, 0.        ,\n",
       "        0.00408098],\n",
       "       [0.        , 0.        , 0.        , ..., 0.00844492, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.02721744, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.01036725, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.01036725, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.01036725, 0.        ,\n",
       "        0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(198, 256), dtype=float32, numpy=\n",
       "array([[ 0.       , -0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.0281344],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ..., -0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       ...,\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guided_grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\n",
       "array([[9.7283053e-01, 2.7169401e-02],\n",
       "       [6.0301521e-03, 9.9396992e-01],\n",
       "       [2.5263358e-02, 9.7473663e-01],\n",
       "       [3.3427373e-02, 9.6657258e-01],\n",
       "       [3.2015406e-02, 9.6798462e-01],\n",
       "       [1.8334357e-02, 9.8166567e-01],\n",
       "       [4.2183723e-02, 9.5781630e-01],\n",
       "       [4.2502719e-04, 9.9957496e-01],\n",
       "       [5.6901835e-02, 9.4309813e-01],\n",
       "       [1.7459923e-02, 9.8254007e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads2 = tf.reduce_sum(grads[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256,), dtype=float32, numpy=\n",
       "array([ 9.19401878e-04, -5.43291681e-03,  3.58202774e-03, -4.33791545e-04,\n",
       "       -2.99719609e-02, -1.51862344e-02, -3.08527425e-03,  1.16565768e-02,\n",
       "       -9.22885723e-03, -1.04158688e-02,  1.61156245e-02, -4.48019058e-03,\n",
       "        1.19484197e-02,  1.46627296e-02, -8.97300988e-03, -1.50216389e-02,\n",
       "       -5.10398485e-03, -5.48715703e-03, -3.22397910e-02, -5.64692914e-03,\n",
       "       -1.08256321e-02, -2.27625621e-03,  3.43708228e-03,  7.31497817e-03,\n",
       "       -1.33044561e-02,  3.59028019e-03, -3.55631951e-03, -1.89220253e-02,\n",
       "        9.96373594e-04, -2.90575624e-03,  1.30825490e-02,  3.95794585e-03,\n",
       "        1.05610443e-03, -1.01920823e-02,  3.84788052e-03,  6.50050538e-03,\n",
       "       -1.78239564e-03, -7.58798700e-03,  1.51814043e-03, -1.65889866e-03,\n",
       "        9.06379148e-03,  2.68663210e-03,  1.12077799e-02,  7.61941075e-03,\n",
       "       -2.11057924e-02, -1.29729630e-02, -4.84679490e-02,  1.10691478e-02,\n",
       "        1.46593899e-03, -7.29838386e-03,  4.92498092e-03,  6.13817107e-03,\n",
       "       -1.07839927e-02, -1.10674147e-02, -2.48578633e-03, -1.27229025e-03,\n",
       "       -3.51533145e-02, -2.06051907e-03, -4.28711064e-03, -3.49748367e-03,\n",
       "        7.16071576e-03, -9.75527242e-03, -4.14703880e-03,  1.37248626e-02,\n",
       "       -6.41122647e-03, -5.51217142e-03, -2.39385329e-02,  8.26666132e-03,\n",
       "       -3.73993674e-03,  6.03538414e-04, -4.46160585e-02, -6.38693688e-04,\n",
       "       -1.40078189e-02, -2.56400742e-03,  1.94861018e-03, -2.15990320e-02,\n",
       "       -3.10113491e-03, -7.24172220e-03,  2.85302196e-02, -1.10741481e-02,\n",
       "       -4.25058941e-04,  5.80435444e-04, -4.67678253e-03,  1.57155097e-02,\n",
       "        9.96274315e-03, -2.35915054e-02, -1.43075334e-02,  6.16487488e-03,\n",
       "        3.13089043e-03,  2.06390899e-02, -3.12985922e-03,  1.93789936e-04,\n",
       "       -5.34373941e-03,  3.74656753e-03, -1.87524036e-02,  1.81830896e-03,\n",
       "        1.81859862e-02, -2.74724932e-03, -7.56236911e-03, -4.38337307e-03,\n",
       "       -9.41767264e-03, -6.51348429e-03, -3.27053703e-02,  6.73276954e-04,\n",
       "        5.35193179e-03, -4.16893978e-03,  7.90000893e-04, -4.62194998e-03,\n",
       "        9.77018848e-03,  1.00872368e-02, -7.38305831e-03, -6.94939168e-03,\n",
       "       -1.13701802e-02,  6.89134421e-03,  8.78530613e-04, -1.71537884e-02,\n",
       "       -2.30188519e-02,  2.52922787e-03, -3.05457273e-03, -6.54119859e-03,\n",
       "        1.68819800e-02, -1.48296943e-02, -2.43927352e-02,  1.53939351e-02,\n",
       "       -1.19116567e-02,  3.98198143e-03,  2.75204843e-03, -1.46459099e-02,\n",
       "        3.91685124e-03,  2.81134271e-03, -3.37526063e-03,  4.16623987e-03,\n",
       "       -2.51862686e-03,  2.86234776e-04,  4.59306233e-04, -4.35208157e-02,\n",
       "        1.53205749e-02, -1.45824980e-02, -6.04305603e-03, -1.38228945e-03,\n",
       "        6.50457758e-03,  1.39118070e-02, -4.49667573e-02,  3.53593542e-03,\n",
       "       -1.59302279e-02,  2.64038006e-03,  1.28853116e-02,  4.87126550e-03,\n",
       "        4.37114853e-03,  5.59598114e-03, -5.63007966e-03,  4.42587305e-03,\n",
       "        3.34676588e-03,  9.65451915e-03, -3.10417963e-04,  5.18572284e-03,\n",
       "       -8.39520060e-03, -4.89990041e-03,  7.79699162e-03,  6.96086907e-04,\n",
       "       -2.29949672e-02, -3.57502839e-03,  9.06994473e-03, -3.39244981e-03,\n",
       "        1.28734820e-02, -4.69295867e-03, -5.98715246e-03, -4.33075707e-03,\n",
       "       -5.87485207e-04,  4.20697453e-03, -1.00632198e-02,  4.17308277e-03,\n",
       "       -1.95507258e-02,  5.53640770e-03, -5.88301476e-03,  1.59352378e-03,\n",
       "        2.58938433e-03, -1.06274709e-02,  6.44848123e-03, -3.81535897e-03,\n",
       "        1.59442201e-02, -1.87131111e-02, -8.90542381e-03,  4.70335362e-03,\n",
       "        1.33290584e-03, -3.64987878e-04,  2.90864659e-03,  1.23459082e-02,\n",
       "        5.67775778e-03, -2.91408338e-02,  7.00373854e-03,  4.91974875e-04,\n",
       "       -1.84879508e-02,  1.09036239e-02, -1.02073755e-02, -4.00729105e-03,\n",
       "        1.11008938e-02,  2.17857072e-03, -4.29933006e-03,  1.23759836e-03,\n",
       "        8.99471226e-04, -2.74539925e-02, -7.69419735e-03,  6.88993954e-04,\n",
       "       -7.24170636e-03,  7.68206501e-03, -4.39466443e-03,  7.55090825e-03,\n",
       "        4.45123529e-03, -1.29087511e-02,  8.03016359e-04, -8.09668843e-03,\n",
       "       -1.60255115e-02, -8.80187005e-03, -8.67666118e-03, -2.93181613e-02,\n",
       "        4.86110244e-03, -2.42756754e-02, -2.61377022e-02,  8.08904413e-03,\n",
       "        1.12982700e-04,  6.38860976e-03,  1.90110388e-03, -6.24043681e-03,\n",
       "        2.32515577e-03, -9.38568730e-03, -1.08654313e-02, -6.16617873e-03,\n",
       "        1.50201726e-03, -5.41852508e-03, -4.78971936e-03, -1.15325209e-02,\n",
       "        1.77412387e-02, -2.12711506e-02,  8.49512406e-03,  7.83946714e-04,\n",
       "        9.76761803e-05,  8.03282484e-03, -1.14699006e-02, -1.47037627e-03,\n",
       "        1.03354929e-02,  5.40154288e-03, -2.49103317e-03,  5.87208010e-03,\n",
       "       -3.00519960e-03, -7.15546892e-04,  3.08855772e-02, -1.45361442e-02,\n",
       "        3.06666195e-02, -7.99913239e-03, -2.17970577e-03, -1.01880962e-03,\n",
       "       -9.87167284e-03, -3.49765504e-03, -1.42921694e-03,  2.81343963e-02],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 256), dtype=float32, numpy=\n",
       "array([[ 0.00000000e+00, -0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -7.16841081e-03, -2.90070079e-06,  1.14815804e-04],\n",
       "       [ 0.00000000e+00, -0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -1.66194257e-03, -2.63019979e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -6.68529561e-03, -0.00000000e+00,  1.02998386e-03],\n",
       "       ...,\n",
       "       [ 1.11339011e-06, -1.45750323e-06,  0.00000000e+00, ...,\n",
       "        -1.13655493e-04, -0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -1.51291005e-02, -7.52252117e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -4.72998712e-03, -8.46520607e-07,  0.00000000e+00]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(tf.reduce_sum(conv_outputs, axis=(1,)),tf.reduce_sum(grads, axis=(1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-ec22ee05dae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#guided_grads = tf.cast(conv_outputs > 0, \"float32\") * tf.cast(grads > 0, \"float32\") * grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mguided_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# guirded_grads 는 198 * 256 이다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonVenv/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/advanced_activations.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_value, negative_slope, threshold, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m       raise ValueError('max_value of Relu layer '\n\u001b[1;32m    301\u001b[0m                        'cannot be negative value: ' + str(max_value))\n",
      "\u001b[0;32m~/pythonVenv/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m   \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "#guided_grads = tf.cast(conv_outputs > 0, \"float32\") * tf.cast(grads > 0, \"float32\") * grads\n",
    "guided_grads = tf.keras.layers.ReLU(tf.reduce_sum(tf.multiply(conv_outputs,grads),axis=1))\n",
    "# guirded_grads 는 198 * 256 이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_output = tf.reduce_sum(conv_outputs, axis=1)\n",
    "guided_grads_2 = tf.reduce_sum(guided_grads, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
